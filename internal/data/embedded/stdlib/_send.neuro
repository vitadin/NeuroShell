%% Description: Seamless send implementation leveraging variable service and auto-client creation
%% Usage: \send Hello, how are you?
%% Assumes user has activated a model first (seamless "Model â†’ Chat" workflow)
%% 
%% Workflow:
%% 1. Ensure active session (create if needed)
%% 2. Add user message to session  
%% 3. Ensure active model (create default if needed)
%% 4. Get client (leverages auto-creation from model commands)
%% 5. Make LLM call
%% 6. Add assistant response and display

%% Step 1: Ensure active session - create if session-activate returns empty
\silent \session-activate
\silent \if-not[condition="${_session_id}"] \session-new

%% Step 2: Add user message to session
\silent \session-add-usermsg[session=${_session_id}] ${_1}

%% Step 3: Ensure active model - create default if no active model
\silent \get[#active_model_name]
\silent \if-not[condition="${#active_model_name}"] \model-new[catalog_id="O4M"] default_model

%% Step 4: Ensure client is available 
%% Since we have an active model, we should have a client ID available
%% If not, we need to get/create the client for the active model
\silent \get[_client_id]
\silent \if-not[condition="${_client_id}"] \echo "Warning: No client ID found, model may not be properly activated"

%% Step 5: Make LLM call using all components
\silent \llm-call[client_id=${_client_id}, session_id=${_session_id}]

%% Step 6: Add assistant response to session and display
\silent \session-add-assistantmsg[session=${_session_id}] ${#llm_response}
\silent \set[1="${#llm_response}"]
\render-markdown ${#llm_response}